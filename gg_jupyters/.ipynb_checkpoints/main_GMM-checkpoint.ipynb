{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************************\n",
    "# Main file in cytokine-identification process. \n",
    "#\n",
    "# Master Thesis, KTH , Gabriel Andersson \n",
    "# ************************************************************\n",
    "import sys\n",
    "import os,sys,inspect\n",
    "\n",
    "sys.path.insert(1,'../')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_functions_wf import *\n",
    "from main_functions import *\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from event_rate_first import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading matlab waveforms files...\n",
      "\n",
      "waveforms loaded succesfully...\n",
      "\n",
      "Shape of waveforms: (136259, 141).\n",
      "\n",
      "Standardize waveforms...\n",
      "\n",
      "\n",
      "Loading matlab timestamps file...\n",
      "\n",
      "timestamps loaded succesfully...\n",
      "\n",
      "Shape of timestamps: (136259, 1).\n",
      "Shape of training data: (13100, 141)\n"
     ]
    }
   ],
   "source": [
    "# ************************************************************\n",
    "# ******************** Parameters ****************************\n",
    "# ************************************************************\n",
    "\n",
    "# VAE training params:\n",
    "continue_train = False\n",
    "nr_epochs = 100 # if all train data is used -- almost no loss-decrease after 100 batches..\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "view_vae_result = False # True => reqires user to give input if to continue the script to pdf-GD or not.. \n",
    "view_GD_result = False # This reqires user to give input if to continue the script to clustering or not.\n",
    "\n",
    "run_DBscan = False\n",
    "run_KMeans = False\n",
    "\n",
    "\n",
    "verbose = 2\n",
    "\n",
    "# pdf GD params: \n",
    "m=0 # Number of steps \n",
    "gamma=0.01 # learning_rate\n",
    "eta=0.01 # Noise variable -- adds white noise with variance eta to datapoints during GD.\n",
    "\n",
    "\n",
    "# DBSCAN params\n",
    "db_eps = 0.2 # max_distance to be considered as neighbours \n",
    "db_min_sample = 200 # Minimum members in neighbourhood to not be regarded as Noise.\n",
    "\n",
    "# Shape of waveforms: (136259, 141)\n",
    "#training_idx = np.arange(10000) # initial testing\n",
    "training_idx = np.arange(0,131000,10)\n",
    "\n",
    "# ************************************************************\n",
    "# ******************** Paths ****************************\n",
    "# ************************************************************\n",
    "\n",
    "path_to_wf = '../../matlab_files/gg_waveforms-R10_IL1B_TNF_03.mat' \n",
    "path_to_ts = '../../matlab_files/gg_timestamps.mat'\n",
    "\n",
    "save_figure = '../figures/deleteme_23nov'\n",
    "# tf weight-file:\n",
    "path_to_weights = '../models/deleteme_23nov'\n",
    "# Numpy file:\n",
    "path_to_hpdp = \"../../numpy_files/numpy_hpdp/deleteme_23nov\"\n",
    "\n",
    "# ************************************************************\n",
    "# ******************** Load Files ****************************\n",
    "# ************************************************************\n",
    "load_data = True\n",
    "if load_data:\n",
    "    waveforms, mean, std = load_waveforms(path_to_wf,'waveforms',standardize=True, verbose=1)\n",
    "    timestamps = load_timestamps(path_to_ts,'gg_timestamps',verbose=1)\n",
    "    \n",
    "    # Extract Training data:\n",
    "    wf_train = waveforms[training_idx]\n",
    "    ts_train = timestamps[training_idx]\n",
    "    #wf_train = waveforms\n",
    "    #ts_train = timestamps\n",
    "    print(f'Shape of training data: {wf_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 96.1744 - reconstruction_loss: 94.1870 - kl_loss: 1.9874\n",
      "Epoch 2/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 81.1388 - reconstruction_loss: 79.2081 - kl_loss: 1.9307\n",
      "Epoch 3/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 79.1001 - reconstruction_loss: 77.1001 - kl_loss: 1.9999\n",
      "Epoch 4/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 77.9043 - reconstruction_loss: 75.8338 - kl_loss: 2.0705\n",
      "Epoch 5/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 77.0231 - reconstruction_loss: 74.8990 - kl_loss: 2.1240\n",
      "Epoch 6/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 76.6891 - reconstruction_loss: 74.5407 - kl_loss: 2.1483\n",
      "Epoch 7/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 76.2974 - reconstruction_loss: 74.1176 - kl_loss: 2.1798\n",
      "Epoch 8/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 76.0358 - reconstruction_loss: 73.8469 - kl_loss: 2.1889\n",
      "Epoch 9/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 75.7710 - reconstruction_loss: 73.5564 - kl_loss: 2.2146\n",
      "Epoch 10/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 75.5776 - reconstruction_loss: 73.3526 - kl_loss: 2.2250\n",
      "Epoch 11/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 75.5023 - reconstruction_loss: 73.2764 - kl_loss: 2.2259\n",
      "Epoch 12/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 75.3443 - reconstruction_loss: 73.0844 - kl_loss: 2.2599\n",
      "Epoch 13/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 75.1213 - reconstruction_loss: 72.8590 - kl_loss: 2.2622\n",
      "Epoch 14/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 74.9829 - reconstruction_loss: 72.7028 - kl_loss: 2.2801\n",
      "Epoch 15/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 74.8858 - reconstruction_loss: 72.5903 - kl_loss: 2.2955\n",
      "Epoch 16/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 74.8088 - reconstruction_loss: 72.4950 - kl_loss: 2.3137\n",
      "Epoch 17/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 74.8070 - reconstruction_loss: 72.4719 - kl_loss: 2.3352\n",
      "Epoch 18/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 74.6212 - reconstruction_loss: 72.2730 - kl_loss: 2.3482\n",
      "Epoch 19/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 74.5366 - reconstruction_loss: 72.1660 - kl_loss: 2.3706\n",
      "Epoch 20/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 74.4123 - reconstruction_loss: 72.0230 - kl_loss: 2.3893\n",
      "Epoch 21/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 74.2516 - reconstruction_loss: 71.8257 - kl_loss: 2.4259\n",
      "Epoch 22/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 74.0857 - reconstruction_loss: 71.6608 - kl_loss: 2.4249\n",
      "Epoch 23/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 74.0663 - reconstruction_loss: 71.6267 - kl_loss: 2.4396\n",
      "Epoch 24/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 73.9754 - reconstruction_loss: 71.5326 - kl_loss: 2.4429\n",
      "Epoch 25/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 74.0629 - reconstruction_loss: 71.6151 - kl_loss: 2.4478\n",
      "Epoch 26/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 73.9470 - reconstruction_loss: 71.4940 - kl_loss: 2.4530\n",
      "Epoch 27/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 73.6109 - reconstruction_loss: 71.1310 - kl_loss: 2.4799\n",
      "Epoch 28/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 73.3856 - reconstruction_loss: 70.8829 - kl_loss: 2.5027\n",
      "Epoch 29/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 73.3567 - reconstruction_loss: 70.8584 - kl_loss: 2.4983\n",
      "Epoch 30/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 73.2299 - reconstruction_loss: 70.7134 - kl_loss: 2.5165\n",
      "Epoch 31/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 73.2280 - reconstruction_loss: 70.7100 - kl_loss: 2.5180\n",
      "Epoch 32/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 73.3442 - reconstruction_loss: 70.8274 - kl_loss: 2.5168\n",
      "Epoch 33/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 73.2504 - reconstruction_loss: 70.7181 - kl_loss: 2.5323\n",
      "Epoch 34/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 73.0342 - reconstruction_loss: 70.4711 - kl_loss: 2.5631\n",
      "Epoch 35/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 72.9101 - reconstruction_loss: 70.3419 - kl_loss: 2.5682\n",
      "Epoch 36/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 72.9342 - reconstruction_loss: 70.3567 - kl_loss: 2.5775\n",
      "Epoch 37/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 72.7382 - reconstruction_loss: 70.1678 - kl_loss: 2.5704\n",
      "Epoch 38/100\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 72.6751 - reconstruction_loss: 70.0918 - kl_loss: 2.5833\n",
      "Epoch 39/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 72.5294 - reconstruction_loss: 69.9288 - kl_loss: 2.6006\n",
      "Epoch 40/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 72.5228 - reconstruction_loss: 69.9303 - kl_loss: 2.5925\n",
      "Epoch 41/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 72.5331 - reconstruction_loss: 69.9365 - kl_loss: 2.5967\n",
      "Epoch 42/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 72.5243 - reconstruction_loss: 69.9074 - kl_loss: 2.6169\n",
      "Epoch 43/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 72.4069 - reconstruction_loss: 69.7813 - kl_loss: 2.6256\n",
      "Epoch 44/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 72.3499 - reconstruction_loss: 69.7230 - kl_loss: 2.6268\n",
      "Epoch 45/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 72.1980 - reconstruction_loss: 69.5559 - kl_loss: 2.6421\n",
      "Epoch 46/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 72.1493 - reconstruction_loss: 69.5067 - kl_loss: 2.6426\n",
      "Epoch 47/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 72.0998 - reconstruction_loss: 69.4527 - kl_loss: 2.6471\n",
      "Epoch 48/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 72.0858 - reconstruction_loss: 69.4310 - kl_loss: 2.6548\n",
      "Epoch 49/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 72.0160 - reconstruction_loss: 69.3595 - kl_loss: 2.6565\n",
      "Epoch 50/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.8835 - reconstruction_loss: 69.2105 - kl_loss: 2.6730\n",
      "Epoch 51/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 71.9926 - reconstruction_loss: 69.3255 - kl_loss: 2.6671\n",
      "Epoch 52/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 72.3320 - reconstruction_loss: 69.6612 - kl_loss: 2.6708\n",
      "Epoch 53/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.8629 - reconstruction_loss: 69.1842 - kl_loss: 2.6787\n",
      "Epoch 54/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 71.7744 - reconstruction_loss: 69.0917 - kl_loss: 2.6828\n",
      "Epoch 55/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.6975 - reconstruction_loss: 69.0035 - kl_loss: 2.6940\n",
      "Epoch 56/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 71.7765 - reconstruction_loss: 69.0801 - kl_loss: 2.6964\n",
      "Epoch 57/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 71.5614 - reconstruction_loss: 68.8570 - kl_loss: 2.7044\n",
      "Epoch 58/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.6061 - reconstruction_loss: 68.8849 - kl_loss: 2.7211\n",
      "Epoch 59/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 71.4931 - reconstruction_loss: 68.7746 - kl_loss: 2.7185\n",
      "Epoch 60/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.3576 - reconstruction_loss: 68.6457 - kl_loss: 2.7120\n",
      "Epoch 61/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.3417 - reconstruction_loss: 68.6251 - kl_loss: 2.7166\n",
      "Epoch 62/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.3414 - reconstruction_loss: 68.6101 - kl_loss: 2.7313\n",
      "Epoch 63/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.2855 - reconstruction_loss: 68.5682 - kl_loss: 2.7173\n",
      "Epoch 64/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.1546 - reconstruction_loss: 68.4285 - kl_loss: 2.7261\n",
      "Epoch 65/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.3285 - reconstruction_loss: 68.6041 - kl_loss: 2.7244\n",
      "Epoch 66/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.1421 - reconstruction_loss: 68.4053 - kl_loss: 2.7368\n",
      "Epoch 67/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.0573 - reconstruction_loss: 68.3260 - kl_loss: 2.7312\n",
      "Epoch 68/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.3522 - reconstruction_loss: 68.5936 - kl_loss: 2.7586\n",
      "Epoch 69/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.2545 - reconstruction_loss: 68.4939 - kl_loss: 2.7607\n",
      "Epoch 70/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.1249 - reconstruction_loss: 68.3816 - kl_loss: 2.7433\n",
      "Epoch 71/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.3527 - reconstruction_loss: 68.5958 - kl_loss: 2.7568\n",
      "Epoch 72/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.3895 - reconstruction_loss: 68.6231 - kl_loss: 2.7663\n",
      "Epoch 73/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.2168 - reconstruction_loss: 68.4556 - kl_loss: 2.7612\n",
      "Epoch 74/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.3171 - reconstruction_loss: 68.5580 - kl_loss: 2.7591\n",
      "Epoch 75/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.0648 - reconstruction_loss: 68.2882 - kl_loss: 2.7766\n",
      "Epoch 76/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 70.8972 - reconstruction_loss: 68.1254 - kl_loss: 2.7718\n",
      "Epoch 77/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.0177 - reconstruction_loss: 68.2545 - kl_loss: 2.7632\n",
      "Epoch 78/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 71.0173 - reconstruction_loss: 68.2363 - kl_loss: 2.7810\n",
      "Epoch 79/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 70.9940 - reconstruction_loss: 68.2174 - kl_loss: 2.7765\n",
      "Epoch 80/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 70.9782 - reconstruction_loss: 68.1978 - kl_loss: 2.7804\n",
      "Epoch 81/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 70.9949 - reconstruction_loss: 68.2167 - kl_loss: 2.7781\n",
      "Epoch 82/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.0478 - reconstruction_loss: 68.2816 - kl_loss: 2.7662\n",
      "Epoch 83/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.0793 - reconstruction_loss: 68.3004 - kl_loss: 2.7789\n",
      "Epoch 84/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 70.9687 - reconstruction_loss: 68.1653 - kl_loss: 2.8034\n",
      "Epoch 85/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 71.0169 - reconstruction_loss: 68.2236 - kl_loss: 2.7932\n",
      "Epoch 86/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 71.0203 - reconstruction_loss: 68.2236 - kl_loss: 2.7967\n",
      "Epoch 87/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.2184 - reconstruction_loss: 68.4401 - kl_loss: 2.7783\n",
      "Epoch 88/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.1122 - reconstruction_loss: 68.3419 - kl_loss: 2.7704\n",
      "Epoch 89/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.1960 - reconstruction_loss: 68.3952 - kl_loss: 2.8008\n",
      "Epoch 90/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 70.9953 - reconstruction_loss: 68.1981 - kl_loss: 2.7972\n",
      "Epoch 91/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 70.9576 - reconstruction_loss: 68.1497 - kl_loss: 2.8079\n",
      "Epoch 92/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 70.8522 - reconstruction_loss: 68.0514 - kl_loss: 2.8007\n",
      "Epoch 93/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.0006 - reconstruction_loss: 68.1820 - kl_loss: 2.8186\n",
      "Epoch 94/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 71.0170 - reconstruction_loss: 68.2082 - kl_loss: 2.8089\n",
      "Epoch 95/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 71.0111 - reconstruction_loss: 68.1893 - kl_loss: 2.8217\n",
      "Epoch 96/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 70.5998 - reconstruction_loss: 67.7814 - kl_loss: 2.8185\n",
      "Epoch 97/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 70.8089 - reconstruction_loss: 67.9703 - kl_loss: 2.8386\n",
      "Epoch 98/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 70.7127 - reconstruction_loss: 67.8720 - kl_loss: 2.8406\n",
      "Epoch 99/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 70.7667 - reconstruction_loss: 67.9358 - kl_loss: 2.8310\n",
      "Epoch 100/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 70.7140 - reconstruction_loss: 67.8865 - kl_loss: 2.8276A: 0s - loss: 70.8481 - reconstruction_loss: 68.0213 - kl_loss: 2.826\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'saved_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2be6009541d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgmm_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnr_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgmm_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saved_weights' is not defined"
     ]
    }
   ],
   "source": [
    "continue_train = True\n",
    "\n",
    "waveform_shape = waveforms.shape[-1]\n",
    "encoder,decoder,gmm_vae = get_gmm_vae(waveform_shape,2)\n",
    "\n",
    "history = gmm_vae.fit(wf_train, epochs=nr_epochs, batch_size=128,verbose=1)\n",
    "gmm_vae.save_weights(saved_weights)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show() \n",
    "print()\n",
    "print(f'Loading {path_to_weights}...')\n",
    "print()\n",
    "vae.load_weights(path_to_weights)\n",
    "if continue_train == True:\n",
    "    print()\n",
    "    print(f'Continue training for {nr_epochs} epochs...')\n",
    "    print()\n",
    "    history = gmm_vae.fit(wf_train, epochs=nr_epochs, batch_size=128,verbose=1)\n",
    "    gmm_vae.save_weights(path_to_weights)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show() \n",
    "#print()\n",
    "#print(f'Visualising decoded latent space...')\n",
    "#print()\n",
    "#plot_decoded_latent(decoder,saveas=save_figure+'_decoded',verbose=1)\n",
    "#plot_encoded(encoder, x_train, saveas=None, verbose=1)\n",
    "z_mean,_,zz = encoder.predict(x_train)\n",
    "\n",
    "'''\n",
    "gmm_components = 5\n",
    "Gmm = Latent_GMM(gmm_components,covariance_type='full')\n",
    "Gmm.fit(zz)\n",
    "g_means = Gmm.get_gaussian_means()\n",
    "g_var = Gmm.get_gaussian_var()\n",
    "probs = Gmm.soft_assignment(zz)\n",
    "labels = Gmm.classify(zz)\n",
    "\n",
    "def plot_gmm(data,labels):\n",
    "    print()\n",
    "    print(f'Visualising gaussian mixture model...')\n",
    "    print()\n",
    "    for ii in range(gmm_components):\n",
    "        plt.scatter(data[labels==ii][::2,0],data[labels==ii][::2,1])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_gmm(zz,labels)\n",
    "\n",
    "print('Done.')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBS the below is not compatible with GMM-syntax yet..:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************************\n",
    "# ******************** Train/Load model **********************\n",
    "# ************************************************************\n",
    "print()\n",
    "print('*********************** Tensorflow Blaj *************************************')\n",
    "print()\n",
    "\n",
    "encoder,decoder,vae = (wf_train, nr_epochs=nr_epochs, batch_size=batch_size, path_to_weights=path_to_weights, \n",
    "                                        continue_train=continue_train, verbose=1)\n",
    "print()\n",
    "print('******************************************************************************')\n",
    "print()\n",
    "\n",
    "#view_vae_result = False # This reqires user to give input if to continue the script to GD or not.\n",
    "if view_vae_result:\n",
    "    plot_decoded_latent(decoder,saveas=save_figure+'_decoded',verbose=1)\n",
    "    continue_to_run_GD = input('Continue to gradient decent of pdf? (yes/no) :')\n",
    "\n",
    "    all_fine = False\n",
    "    while all_fine==False:\n",
    "        if continue_to_run_GD=='no':\n",
    "            exit()\n",
    "        elif continue_to_run_GD=='yes':\n",
    "            print('Continues to \"run_GD\"')\n",
    "            all_fine = True\n",
    "        else:\n",
    "            continue_to_run_GD = input('Invalid input, continue to gradient decent of pdf? (yes/no) :')\n",
    "\n",
    "\n",
    "# ************************************************************\n",
    "# ** Perform GD on pdf to find high prob. data-points (hpdp) *\n",
    "# ************************************************************  \n",
    "\n",
    "run_GD = True\n",
    "#view_GD_result = True # This reqires user to give input if to continue the script to clustering or not.\n",
    "if run_GD:\n",
    "    print()\n",
    "    print('Running pdf_GD to get hpdp...')\n",
    "    print()\n",
    "    # To easy computational load -- only every 20th data-point is used..\n",
    "    hpdp = pdf_GD(vae, wf_train[0::20], m=m, gamma=gamma, eta=eta, path_to_hpdp=path_to_hpdp,verbose=verbose)\n",
    "    #hpdp = pdf_GD(vae, wf_train, m=m, gamma=gamma, eta=eta, path_to_hpdp=path_to_hpdp,verbose=verbose)\n",
    "\n",
    "    if view_GD_result:\n",
    "        print(f'Visualising decoded latent space of hpdp...')\n",
    "        print()\n",
    "        plot_encoded(encoder, hpdp, saveas=save_figure+'_hpdp_encoded', verbose=1)        \n",
    "        continue_to_Clustering = input('Continue to Clustering? (yes/no) :')\n",
    "\n",
    "        all_fine = False\n",
    "        while all_fine==False:\n",
    "            if continue_to_Clustering=='no':\n",
    "                exit()\n",
    "            elif continue_to_Clustering=='yes':\n",
    "                print('Continues to \"run_GD\"')\n",
    "                all_fine = True\n",
    "            else:\n",
    "                continue_to_Clustering = input('Invalid input, continue to Clustering? (yes/no) :')\n",
    "\n",
    "else:\n",
    "    print()\n",
    "    print('Skipps over pdf_GD...')\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "# ************************************************************\n",
    "# ******************** Cluster wf using hpdp *****************\n",
    "# ******************** This to access labels *****************\n",
    "# ************************************************************\n",
    "\n",
    "\n",
    "if run_KMeans:\n",
    "    print()\n",
    "    print('Running KMeans on hpdp...')\n",
    "    print()\n",
    "    kmeans = KMeans(n_clusters=10, random_state=0).fit(hpdp)\n",
    "    labels = kmeans.labels_\n",
    "else:\n",
    "    print()\n",
    "    print('Skipps over KMeans...')\n",
    "    print()\n",
    "\n",
    "if run_DBscan:\n",
    "    print()\n",
    "    print('Running DBSCAN on hpdp...')\n",
    "    print()\n",
    "    dbscan = DBSCAN(eps=db_eps, min_samples=db_min_sample, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)\n",
    "    hpdp_latent_mean,_,_ = encoder.predict(hpdp)\n",
    "    dbscan.fit(hpdp_latent_mean)\n",
    "    labels = dbscan.labels_ #  Noisy samples are given the label -1\n",
    "else:\n",
    "    print()\n",
    "    print('Skipps over DBSCAN...')\n",
    "    print()\n",
    "\n",
    "\n",
    "# ************************************************************\n",
    "# ******************** Event Rate *****************\n",
    "# ************************************************************\n",
    "run_event_rate = True\n",
    "if run_event_rate:\n",
    "    print('Calculating Event rates...')\n",
    "    event_rates, real_clusters = get_event_rates(ts_train[0::20],labels,bin_width=1)\n",
    "    delta_ev, ev_stats = delta_ev_measure(event_rates,timestamps,labels)\n",
    "\n",
    "    #event_rates, real_clusters = get_event_rates(ts_train,labels,bin_width=1)\n",
    "    print(f'Real cluster (with mean event_rate over 0.5 is CAPs {real_clusters})')\n",
    "    plot_event_rates(event_rates,ts_train[0::20],saveas=save_figure+'_event_rate', conv_width=30)\n",
    "    #plot_event_rates(event_rates,ts_train,saveas=save_figure+'_event_rate', conv_width=30)\n",
    "\n",
    "# ************************************************************\n",
    "# ******************** General PLOTTING ******************************\n",
    "# ************************************************************\n",
    "# Plot hpdp:\n",
    "if verbose>1:\n",
    "    print()\n",
    "    print(f'Plotting waveforms of each cluster if labels are specified...')\n",
    "    print()\n",
    "    plot_waveforms(hpdp,labels=None)\n",
    "    print()\n",
    "    print(f'Visualising decoded latent space of hpdp...')\n",
    "    print()\n",
    "    plot_encoded(encoder, hpdp, saveas=save_figure+'_hpdp_encoded', verbose=verbose)\n",
    "    print()\n",
    "    print(f'Visualising decoded latent space...')\n",
    "    print()\n",
    "    plot_decoded_latent(decoder,saveas=save_figure+'_decoded',verbose=1)\n",
    "    plot_encoded(encoder, wf_train, saveas=save_figure+'_encoded', verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
